# -*- coding: utf-8 -*-
"""cashflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18STDLQTSeXHXBbrmE4Y6U0HEZByd886N
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from statistics import mean
from sklearn.metrics import mean_squared_error as mse
import math

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression,LinearRegression
from sklearn import metrics

from xgboost import XGBRegressor

from keras.models import Sequential 
from keras.layers import LSTM, Dense, Dropout 
from keras.callbacks import EarlyStopping 
from sklearn.preprocessing import RobustScaler, MinMaxScaler 
from sklearn.feature_selection import SelectKBest,chi2,RFE

pd.set_option('display.max_columns', 100)
sns.set_style('darkgrid')



"""**Import dataset**"""

URL = 'https://drive.google.com/file/d/1YVT57kBOTaliCH30hYG_xFZ3kC_FABfr/view?usp=sharing'
path = 'https://drive.google.com/uc?ex}port=download&id='+URL.split('/')[-2]
df = pd.read_csv(path)
df.head()

"""**Data Cleaning**"""

print("shape of dataset before cleaning:", df.shape)
print(df.dtypes)
df.isna().any()
df = df.dropna()
print("shape of dataset after cleaning:", df.shape)

# Change Date format to time series
df['SettledDate'] = pd.to_datetime(df['SettledDate'], format='%d/%m/%Y')
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d/%m/%Y')
df['DueDate'] = pd.to_datetime(df['DueDate'], format='%d/%m/%Y')

# Time gap between settled date and invoice date
df['Settle_Invoice']= df.SettledDate - df.InvoiceDate
df['Settle_Invoice'] = df['Settle_Invoice'].dt.days

# Time gap between settled date and due date
df['Settle_Due']= df.SettledDate - df.DueDate
df['Settle_Due'] = df['Settle_Due'].dt.days

# Cashflow
# Convert all invoice amount to USD

df[df["InvoiceCurrency"]=="AUD"]["InvoiceAmount"] = df[df["InvoiceCurrency"]=="AUD"]["InvoiceAmount"]*0.75
df[df["InvoiceCurrency"]=="CHF"]["InvoiceAmount"] = df[df["InvoiceCurrency"]=="CHF"]["InvoiceAmount"]*1.08
df[df["InvoiceCurrency"]=="EUR"]["InvoiceAmount"] = df[df["InvoiceCurrency"]=="EUR"]["InvoiceAmount"]*1.11
df[df["InvoiceCurrency"]=="JPY"]["InvoiceAmount"] = df[df["InvoiceCurrency"]=="JPY"]["InvoiceAmount"]*0.0082
df[df["InvoiceCurrency"]=="SGD"]["InvoiceAmount"] = df[df["InvoiceCurrency"]=="SGD"]["InvoiceAmount"]*0.74

df["InvoiceAmountUSD"] = df["InvoiceAmount"]

df["cashflow"] = df["SettledAmountUSD"] - df["InvoiceAmountUSD"]
df

df1 = df.set_index('InvoiceDate')

# to_datetime() method converts string
# format to a DateTime object
df1.index = pd.to_datetime(df1.index)
max(df1.index)
# dates which are not in the sequence
# are returned
print(pd.date_range(start="2018-01-03", end="2020-10-24").difference(df1.index))

idx = pd.date_range(start="2018-01-03", end="2020-10-24")
# df1.reindex(idx, fill_value=0)

"""We first perform some data cleaning, such as formalisation of the datatime format and imputation of missing values."""

df_partner = df.groupby("Partner").mean()
df_partner

df_partner["Invoice Count"]= df.groupby('Partner').size()

df_partner

corrMatrix = df_partner.corr()
sns.heatmap(corrMatrix, annot=True)
plt.show()

"""**EDA**"""

sns.distplot(df_partner['Settle_Due'], color= 'navy', hist=False)

sns.distplot(df_partner['SettlementAmount'], color= 'navy', hist=False)

sns.distplot(df['SettledAmountUSD'], color= 'navy', hist=False)

sns.distplot(df['Settle_Invoice'], color= 'navy', hist=False)

sns.distplot(df['Settle_Due'], color= 'navy', hist=False)

sns.distplot(df['PaymentTerms'], color= 'navy', hist=False)

sns.distplot(df_partner['cashflow'], color= 'navy', hist=False)

# set the dataframe index as invoice_date
# There are duplicates in invoiceDate, we take mean Settle_Invoice

df_new = df.groupby('InvoiceDate').agg({'Settle_Invoice': ['mean'],'Settle_Due': ['mean'],'PaymentTerms':['mean']})
df_new.plot(figsize=(18,6))

df_new_partner = df.groupby('Partner').agg({'Settle_Invoice': ['mean'],'Settle_Due': ['mean'],'PaymentTerms':['mean']})
df_new_partner.plot()

df_new_partner

percentage_of_late_invoices = len(df[df["Settle_Due"]>0])/len(df)*100
percentage_of_late_invoices

mean(df["Settle_Due"])

mean(df_partner["Invoice Count"])

df['CurrencyPair'].unique()

df["InvoiceCurrency"].unique()

#top 5 firms most likely to pay late
df_partner.sort_values("Settle_Due",ascending=False).head(5).index

"""1. Across all the invoices listed, 37.6% of them are late. 
2. Across all the invoices, the average date of payment is two days behind the due date. 
3. On average, each partner has 43 invoices.
4. The top five firms most likely to pay late are 'Hi-Tech Distribution Pte Ltd', 'Toll Global Forwarding', 'Daily Fresh',  'Isewan Terminal Service' and 'Gondrand SPA'.

**Logistic regression**

I use logistic regression to predict whether a transaction will be overdue or not. Hope this information will be useful haha.
"""

# # Select last 14 days for prediction
df_test = df[(df["InvoiceDate"].max() - df["InvoiceDate"]).dt.days <= 14]
df_train = df[(df["InvoiceDate"].max() - df["InvoiceDate"]).dt.days > 14]
print("df_train shape: ", df_train.shape)
print("df_test shape: ", df_test.shape)

df_train['OverDue'] = np.where(df_train['Settle_Due'] < 0 , 1, 0)

df_dummies = pd.get_dummies(df_train)

features= ['Country','Partner','ParternType','Bank','PaymentTerms','Settle_Invoice']

X = pd.get_dummies(df_train[features])
Y = df_dummies['OverDue']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

model = LogisticRegression()
result = model.fit(X_train, Y_train)

prediction_test = model.predict(X_test)

# Print the prediction accuracy
print ("Accuracy score: ", metrics.accuracy_score(Y_test, prediction_test))

np.mean((model.predict(X_test) > 0.5) == Y_test)

"""**XGBoost**"""

# transform a time series dataset into a supervised learning dataset
# 1 day shift = 1 day horizon
def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	df = data
	cols = list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(df.shift(i))
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(df.shift(-i))
	# put it all together
	agg = pd.concat(cols, axis=1)
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg.values

def train_test_split(data, n_test):
	return data[:-n_test, :], data[-n_test:, :]

def xgboost_forecast(train, testX):
# transform list into array
  train = np.asarray(train)
# split into input and output columns
  trainX, trainy = train[:, :-1], train[:, -1]
# fit model	
  model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)
  model.fit(trainX, trainy)
# make a one-step prediction
  yhat = model.predict(np.asarray([testX]))
  return yhat[0]

# walk-forward validation for univariate data
def walk_forward_validation(data, n_test):
	predictions = list()
	# split dataset
	train, test = train_test_split(data, n_test)
	# seed history with training dataset
	history = [x for x in train]
	# step over each time-step in the test set
	for i in range(len(test)):
		# split test row into input and output columns
		testX, testy = test[i, :-1], test[i, -1]
		# fit model on history and make a prediction
		yhat = xgboost_forecast(history, testX)
		# store forecast in list of predictions
		predictions.append(yhat)
		# add actual observation to history for the next loop
		history.append(test[i])
		# summarize progress
		print('>expected=%.1f, predicted=%.1f' % (testy, yhat))
	# estimate prediction error
	rmse = math.sqrt(mse(test[:, -1], predictions))
	return rmse, test[:, -1], predictions

# evaluate
data = series_to_supervised(df[["Settle_Invoice"]],n_in=1, n_out=1)
rmse, y, yhat = walk_forward_validation(data, 14)
print('RMSE: %.3f' % rmse)

# evaluate
data = series_to_supervised(df[["Settle_Invoice"]],n_in=2, n_out=2)
rmse, y, yhat = walk_forward_validation(data, 14)
print('RMSE: %.3f' % rmse)

# evaluate
data = series_to_supervised(df[["Settle_Invoice"]],n_in=3, n_out=3)
rmse, y, yhat = walk_forward_validation(data, 14)
print('RMSE: %.3f' % rmse)

# evaluate
data = series_to_supervised(df[["Settle_Due"]],n_in=3, n_out=3)
rmse, y, yhat = walk_forward_validation(data, 14)
print('RMSE: %.3f' % rmse)

data = series_to_supervised(df[["cashflow"]],n_in=3, n_out=3)
rmse, y, yhat = walk_forward_validation(data, 14)
print('RMSE: %.3f' % rmse)

"""**LSTM**

https://www.relataly.com/stock-market-prediction-using-multivariate-time-series-in-python/1815/
"""

features= ['Country','Partner','ParternType','Bank','PaymentTerms','Settle_Invoice']

X = pd.get_dummies(df[features])
Y = df[['Settle_Invoice']]

# Feature extraction
model = LinearRegression()
rfe = RFE(model,n_features_to_select=10)
fit = rfe.fit(X, Y)
print("Num Features: %s" % (fit.n_features_))

newX = X[fit.get_feature_names_out()]
print(fit.get_feature_names_out())
newX.head()

# Set the sequence length - this is the timeframe used to make a single prediction
sequence_length = 50
np_data_scaled = newX
# Prediction Index
index_Close = np_data_scaled.columns.get_loc("Settle_Invoice")

train_data_len = np_data_scaled.shape[0] - 14

train_data = np_data_scaled.iloc[0:train_data_len, :]
test_data = np_data_scaled.iloc[train_data_len - sequence_length:, :]

# The RNN needs data with the format of [samples, time steps, features]
# Here, we create N samples, sequence_length time steps per sample, and 6 features
def partition_dataset(sequence_length, data):
    x, y = [], []
    data_len = data.shape[0]
    for i in range(sequence_length, data_len):
        x.append(data.iloc[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn
        y.append(data.iloc[i, index_Close]) #contains the prediction values for validation,  for single-step prediction
    
    # Convert the x and y to numpy arrays
    x = np.array(x)
    y = np.array(y)
    return x, y

# Generate training data and test data
x_train, y_train = partition_dataset(sequence_length, train_data)
x_test, y_test = partition_dataset(sequence_length, test_data)

# Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

# Validate that the prediction value and the input match up
# The last close price of the second input sample should equal the first prediction value
print(x_train[1][sequence_length-1][index_Close])
print(y_train[0])

# Configure the neural network model
model = Sequential()

# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables
n_neurons = x_train.shape[1] * x_train.shape[2]
print(n_neurons, x_train.shape[1], x_train.shape[2])
model.add(LSTM(n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2]))) 
model.add(LSTM(n_neurons, return_sequences=False))
model.add(Dense(5))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Training the model
epochs = 5
batch_size = 16
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
history = model.fit(x_train, y_train, 
                    batch_size=batch_size, 
                    epochs=epochs,
                    validation_data=(x_test, y_test)
                   )
                    
                    #callbacks=[early_stop])

y_pred= model.predict(x_test)

# Mean Absolute Error (MAE)
RMSE = math.sqrt(mse(y_test, y_pred))
print(f'RMSE: {np.round(RMSE, 2)}')

"""Auto ARIMA

https://betterprogramming.pub/using-auto-arima-with-python-e482e322f430
"""

!pip install numpy --upgrade
!pip install scipy --upgrade
!pip install sklearn --upgrade
!pip install pandas --upgrade

!pip install pmdarima
from pmdarima.arima import auto_arima

df

univariate_ser = df.set_index("InvoiceDate")["Settle_Due"]
univariate_ser

# Train Test Split Index
train_size = 0.8
split_idx = round(len(univariate_ser)* train_size)
split_idx

# Split
train = univariate_df.iloc[:split_idx]
test = univariate_df.iloc[split_idx:]

# Visualize split
fig,ax= plt.subplots(figsize=(12,8))
kws = dict(marker='o')
plt.plot(train, label='Train', **kws)
plt.plot(test, label='Test', **kws)
ax.legend(bbox_to_anchor=[1,1]);

arima_model = auto_arima(train, start_p=0, start_q=0)
arima_model.summary()

arima_model.plot_diagnostics()

!pip install statsmodels --upgrade
def forecast_to_df(model, steps=12):
    forecast = model.get_forecast(steps=steps)
    pred_df = forecast.conf_int()
    pred_df['pred'] = forecast.predicted_mean
    pred_df.columns = ['lower', 'upper', 'pred']
    return pred_df

future_forecast = arima_model.predict(n_periods=30)
future_forecast

pred= arima_model.predict(len(test))

# Mean Absolute Error (MAE)
RMSE = math.sqrt(mse(test, pred))
print(f'RMSE: {np.round(RMSE, 2)}')

"""**DeepAR**

https://kekayan.medium.com/forecasting-with-deepar-for-busy-people-ed67f9d9a00d

https://ts.gluon.ai/tutorials/forecasting/quick_start_tutorial.html
"""

univariate_df = univariate_ser.to_frame()
univariate_df.reset_index(inplace=True)
univariate_df = univariate_df.sort_values(by="InvoiceDate")
univariate_df

univariate_df['InvoiceDate'] = pd.to_datetime(univariate_df['InvoiceDate'], errors = 'coerce')

univariate_df=univariate_df.set_index("InvoiceDate")
univariate_df.tail(500)

univariate_df["Settle_Due"].plot()

!pip install mxnet gluonts ujson
from gluonts.model.deepar import DeepAREstimator
from gluonts.mx.distribution import ZeroInflatedNegativeBinomialOutput, StudentTOutput #likelihood
from gluonts.mx.trainer.learning_rate_scheduler import LearningRateReduction
from gluonts.mx.trainer import Trainer
from gluonts.mx.trainer.model_averaging import ModelAveraging, SelectNBestSoftmax, SelectNBestMean

from gluonts.evaluation import Evaluator
from gluonts.evaluation.backtest import make_evaluation_predictions

from gluonts.dataset.common import ListDataset
from gluonts.dataset.util import to_pandas
training_data = ListDataset(
    [{"start": univariate_df.index[0], "target": univariate_df.Settle_Due[:"2020-07-04T00:00:00.000000"]}],
    freq = "d"
)

test_data = ListDataset(
    [{"start": univariate_df.index[0], "target": univariate_df.Settle_Due[:"2020-10-24T00:00:00.000000"]}],
    freq = "d"
)
test_data

entry = next(iter(training_data))
train_series = to_pandas(entry)
train_series.plot()
plt.grid(which="both")
plt.legend(["train series"], loc="upper left")
# plt.title(entry['item_id'])
plt.show()

callbacks = [
    LearningRateReduction(objective="min",
                          patience=10,
                          base_lr=1e-3,
                          decay_factor=0.5,
                          ),
    ModelAveraging(avg_strategy=SelectNBestMean(num_models=2))
]


estimator = DeepAREstimator(
    freq="d",
    prediction_length=24,
    context_length=36,
    num_layers = 2,
    num_cells = 40,
    distr_output=StudentTOutput(),
    dropout_rate=0.01,
    trainer=Trainer(#ctx = mx.context.gpu(),
                    epochs=500,
                    callbacks=callbacks))

predictor = estimator.train(training_data)

forecast_it, ts_it = make_evaluation_predictions(
    dataset=test_data,  # test dataset
    predictor=predictor,  # predictor
    num_samples=100,  # number of sample paths we want for evaluation
)

forecasts = list(forecast_it)
tss = list(ts_it)
ts_entry = tss[0]
forecast_entry = forecasts[0]
def plot_prob_forecasts(ts_entry, forecast_entry):
    plot_length = 150
    prediction_intervals = (50.0, 90.0)
    legend = ["observations", "median prediction"] + [f"{k}% prediction interval" for k in prediction_intervals][::-1]

    fig, ax = plt.subplots(1, 1, figsize=(10, 7))
    ts_entry[-plot_length:].plot(ax=ax)  # plot the time series
    forecast_entry.plot(prediction_intervals=prediction_intervals, color='g')
    plt.grid(which="both")
    plt.legend(legend, loc="upper left")
    plt.show()

evaluator = Evaluator()
agg_metrics, item_metrics = evaluator(iter(tss), iter(forecasts), num_series=len(test_data))

import json
print(json.dumps(agg_metrics, indent=4))

"""DeepAR seems to perform below expectations. This could be due to the inherent nature of the data set.

**Baseline**
"""

train_date = df.iloc[0:len(df)-14,:]
test_date = df.iloc[-14:,:]

train_date.PaymentTerms
due_avg = mean(train_date.groupby('InvoiceDate').Settle_Due.mean())

test_due = test_date.groupby('InvoiceDate').Settle_Due.mean()
rmse_baseline = math.sqrt(mean((test_due-due_avg)**2))
print("RMSE baseline (if we predict with the avg of overdue time): ",rmse_baseline)

"""**Baseline after Grouping by Partners**


"""

rmse_baseline_groupby_partners = math.sqrt(mean((mean(df_partner["Settle_Due"])-df_partner["Settle_Due"])**2))
print("RMSE baseline (if we predict with the avg of overdue time after grouping by partners): ", rmse_baseline_groupby_partners)

"""The baseline prediction works surprisingly well compared to other models we have built. This suggests the more complex models may not be necessarily suited to this particular problem.

**Clustering**
"""

# clustering based on the number of invoice for each partner
sns.scatterplot(data = df_partner, x = "Invoice Count", y= "Settle_Due", color="black")
plt.xlabel('x')
plt.ylabel('y');

from sklearn.cluster import KMeans

clustering = KMeans().fit(df_partner[["Invoice Count", "Settle_Due"]])
df_partner["cluster"] = clustering.labels_

sns.scatterplot(data = df_partner, x = "Invoice Count", y= "Settle_Due", hue ="cluster", legend = None)

clustering.inertia_

#elbow method
from sklearn.cluster import KMeans 
inertias = [] 
mapping2 = {} 
K = np.arange(1,10)
X = df_partner[["Invoice Count", "Settle_Due"]]
    
for k in K: 
    #Building and fitting the model 
    kmeanModel = KMeans(n_clusters=k).fit(X)     
      
    inertias.append(kmeanModel.inertia_) 
  
    mapping2[k] = kmeanModel.inertia_

plt.plot(K, inertias, 'bx-') 
plt.xlabel('Values of K') 
plt.ylabel('Inertia') 
plt.title('The Elbow Method using Inertia') ;

clustering = KMeans(n_clusters=3).fit(df_partner[["Invoice Count", "Settle_Due"]])

df_partner["cluster2"] = clustering.labels_

sns.scatterplot(data = df_partner, x = "Invoice Count", y= "Settle_Due", hue ="cluster2", legend = None);

# clustering based on SettledAmountUSD	for each partner
sns.scatterplot(data = df_partner, x = "SettledAmountUSD", y= "Settle_Due", color="black")
plt.xlabel('x')
plt.ylabel('y');

clustering = KMeans().fit(df_partner[["SettledAmountUSD", "Settle_Due"]])
df_partner["cluster3"] = clustering.labels_

sns.scatterplot(data = df_partner, x = "SettledAmountUSD", y= "Settle_Due", hue ="cluster3", legend = None)

#elbow method
from sklearn.cluster import KMeans 
inertias = [] 
mapping2 = {} 
K = np.arange(1,10)
X = df_partner[["SettledAmountUSD", "Settle_Due"]]
    
for k in K: 
    #Building and fitting the model 
    kmeanModel = KMeans(n_clusters=k).fit(X)     
      
    inertias.append(kmeanModel.inertia_) 
  
    mapping2[k] = kmeanModel.inertia_

plt.plot(K, inertias, 'bx-') 
plt.xlabel('Values of K') 
plt.ylabel('Inertia') 
plt.title('The Elbow Method using Inertia') ;

clustering = KMeans(n_clusters=2).fit(df_partner[["SettledAmountUSD", "Settle_Due"]])

df_partner["cluster4"] = clustering.labels_

sns.scatterplot(data = df_partner, x = "SettledAmountUSD", y= "Settle_Due", hue ="cluster4", legend = None)

"""There is no obvious link found between the amount of money being settled/the number of invoices for each partner and the date of payment.

**Comparison and Evaluation**

1. Machine learning methods, in particular XGBoost perform decently well with reference to this data set. This could be due to the use of covariates to bolster prediction power. 
2. Models in the time series forecast seem to perform below expectations. We suspect it might have to do with the inherent nature of this data set. 
3. A simple average after grouping by partners in fact performs best in predicting the date of payment.

**Recommendation**

1. Focus on the top few firms with the highest propensity to pay late and take active precautions.
2. When evaluating the overall cash flow, factor in an average of two days in late payment to ensure the financial health. 
3. There is a dip in amount of time taken to pay/loan period towards the end of 2020, which might have impacted our predictions. It could be important to look into this with more contextual data.
"""

